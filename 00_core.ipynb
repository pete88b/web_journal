{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Core tools for working with storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from abc import ABC,abstractmethod\n",
    "from configparser import ConfigParser\n",
    "from pathlib import Path\n",
    "import azure.storage.blob,azure.core.exceptions\n",
    "import boto3\n",
    "import zlib,shutil,re,json,importlib,hashlib,datetime\n",
    "from typing import List,Tuple,Optional,Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import *\n",
    "from configparser import SectionProxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_config(section_name:str=None,config_name:str='secrets/settings.ini'):\n",
    "    \"Read the INI file `config_name` and return a dict for `section_name` if specified\"\n",
    "    config_path=Path(config_name)\n",
    "    config=ConfigParser()\n",
    "    config.read(config_path)\n",
    "    if section_name is None:\n",
    "        return config\n",
    "    if section_name not in config:\n",
    "        raise Exception(f'Error: [{section_name}] section not found in {config_path}')\n",
    "    return dict(config.items(section_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `section_name` is not specified, this function will return the `ConfigParser` used to read the INI file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(read_config(),ConfigParser)\n",
    "assert isinstance(read_config()['DEFAULT'],SectionProxy)\n",
    "assert isinstance(read_config('DEFAULT'),dict)\n",
    "test_eq(read_config('local_cwd',config_name='test/settings.ini')['storage_client'],\n",
    "        'storage_tools.core.LocalStorageClient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_dataset_archive_name(name:str) -> Optional[Tuple[str,...]]:\n",
    "    \"Returns (name,version) if `name` is a dataset archive name, `None` otherwise\"\n",
    "    match = re.match(r'^([\\./\\s\\w-]+)\\.(\\d+\\.\\d+\\.\\d+)\\.zip$',name)\n",
    "    return None if match is None else match.group(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(('dsetname', '0.0.1'), parse_dataset_archive_name('dsetname.0.0.1.zip'))\n",
    "test_eq(('dsetname.txt', '0.2.1'), parse_dataset_archive_name('dsetname.txt.0.2.1.zip'))\n",
    "test_eq(('path/to/dsetname', '0.0.1'), parse_dataset_archive_name('path/to/dsetname.0.0.1.zip'))\n",
    "test_eq(('//path/to/dsetname', '0.0.1'), parse_dataset_archive_name('//path/to/dsetname.0.0.1.zip'))\n",
    "test_eq(None, parse_dataset_archive_name('dsetname.0.0.1.csv'))\n",
    "test_eq(None, parse_dataset_archive_name('dsetname.0.1.zip'))\n",
    "test_eq(None, parse_dataset_archive_name('dsetname.0.a.1.csv'))\n",
    "test_eq(None, parse_dataset_archive_name('.0.0.1.zip'))\n",
    "test_eq(None, parse_dataset_archive_name('0.0.1.csv'))\n",
    "test_eq(None, parse_dataset_archive_name('dsetname.0.0.1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def parse_dataset_archive_version(version:str) -> List[int]:\n",
    "    \"Returns (major,minor,patch) if `version` is a valid dataset archive version\"\n",
    "    match = re.match(r'^(\\d+)\\.(\\d+)\\.(\\d+)$',version)\n",
    "    if match is None: raise ValueError(f'Invalid version: {version}')\n",
    "    return [int(s) for s in match.group(1,2,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq([0,1,2],parse_dataset_archive_version('0.1.2'))\n",
    "test_eq([5,4,3],parse_dataset_archive_version('5.4.3'))\n",
    "test_fail(lambda: parse_dataset_archive_version('0.1.2.'))\n",
    "test_fail(lambda: parse_dataset_archive_version('0.1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def next_version(versions:List[str]=None,increment:str='patch'):\n",
    "    \"Return the version that should follow the last version in `versions`\"\n",
    "    v=[0,0,0] if versions is None else parse_dataset_archive_version(versions[-1])\n",
    "    if increment=='patch': v[2]+=1\n",
    "    elif increment=='minor': v[1]+=1;v[2]=0\n",
    "    elif increment=='major': v[0]+=1;v[1]=0;v[2]=0\n",
    "    else: raise ValueError(f'Unknown increment: {increment}')\n",
    "    return f'{v[0]}.{v[1]}.{v[2]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq('0.0.1',next_version(None))\n",
    "test_eq('33.55.67',next_version(['2.4.60','33.55.66']))\n",
    "test_eq('0.1.0',next_version(None,'minor'))\n",
    "test_eq('1.0.0',next_version(None,'major'))\n",
    "test_eq('3.0.0',next_version(['2.4.60'],'major'))\n",
    "test_fail(lambda: next_version(None,'beta'))\n",
    "test_fail(lambda: next_version(['2.4.60','33.55.66a']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sha256(file:Union[Path,str]) -> str:\n",
    "    \"Return the secure hash (as a hex digest) of the specified file\"\n",
    "    m = hashlib.sha256()\n",
    "    with open(file,'rb') as f: m.update(f.read())\n",
    "    return m.hexdigest()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(sha256('test/test_data.csv'),sha256(Path('test/test_data.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_manifest(archive_folder):\n",
    "    \"Returns (archive folder path, manifest file path, manifest as dict)\"\n",
    "    p=Path(archive_folder)\n",
    "    mf=p/'manifest.json'\n",
    "    if mf.is_file():\n",
    "        with open(mf) as f: m=json.load(f)\n",
    "    else: \n",
    "        m={}\n",
    "    return p,mf,m\n",
    "\n",
    "def make_or_update_manifest(archive_folder:Union[Path,str]):\n",
    "    \"Create or update a manifest in `archive_folder`\"\n",
    "    p,mf,m=_get_manifest(archive_folder)\n",
    "    m['datetime']=datetime.datetime.utcnow().isoformat()\n",
    "    m['files']=[]\n",
    "    len_p=len(str(p).replace('\\\\','/'))\n",
    "    for f in [f for f in p.rglob('*') if f.is_file()]:\n",
    "        m['files'].append(dict(\n",
    "            file=str(f).replace('\\\\','/')[len_p+1:],\n",
    "            sha256=sha256(f)))\n",
    "    with open(mf,'w') as f: json.dump(m,f,indent=2,sort_keys=True)\n",
    "        \n",
    "def check_archive(archive_folder:Union[Path,str]):\n",
    "    \"Check that all files listed in manifest.json have the correct secure hash\"\n",
    "    p,mf,m=_get_manifest(archive_folder)\n",
    "    for file in m['files']:\n",
    "        expected,actual=file['sha256'],sha256(p/file['file'])\n",
    "        if actual!=expected:\n",
    "            raise ValueError(f\"sha mismatch for {file['file']}. Expected {expected} but found {actual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_dataset_archive_folder(\n",
    "        path:str, name:str, versions:List[str]=None, version:str='patch') -> str:\n",
    "    \"Create a new dataset archive folder in `path`\"\n",
    "    src=Path(path)/name\n",
    "    if not src.exists(): raise FileNotFoundError(f'{src} not found')\n",
    "        \n",
    "    if version in ['major','minor','patch']:\n",
    "        version=next_version(versions,version)\n",
    "    else:\n",
    "        parse_dataset_archive_version(version)\n",
    "        \n",
    "    archive_folder=Path(path)/'.'.join([name,version])\n",
    "    if archive_folder.exists(): \n",
    "        raise FileExistsError(f'Archive folder {archive_folder} exists')\n",
    "    if src.is_file(): \n",
    "        archive_folder.mkdir(parents=True)\n",
    "        shutil.copy(src,archive_folder)\n",
    "    else: \n",
    "        shutil.copytree(src,archive_folder)\n",
    "    make_or_update_manifest(archive_folder)\n",
    "    return f'{path}/{name}.{version}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def _rmtree(p):\n",
    "    try: shutil.rmtree(p)\n",
    "    except FileNotFoundError: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config=dict(storage_client='storage_tools.core.LocalStorageClient',\n",
    "                 local_path='test/local_path2',storage_area='test/storage_area2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_local_test_data():\n",
    "    for p in [test_config['local_path'],test_config['storage_area']]: _rmtree(p)\n",
    "    test_files=['a/b/test_data.2.0.0.txt','test_data.txt']\n",
    "    for i in reversed(range(3)): test_files.insert(1,f'sub/test_data.0.0.{i}.txt')\n",
    "    for i,f in enumerate(test_files):\n",
    "        f=test_config['local_path']+'/'+f\n",
    "        Path(f).parent.mkdir(parents=True,exist_ok=True)\n",
    "        with open(f, 'w') as _file: _file.write(f'a little bit of data {i}')\n",
    "    return test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_make_local_test_data()\n",
    "\n",
    "test_eq(test_config['local_path']+'/test_data.txt.0.0.1',\n",
    "        make_dataset_archive_folder(test_config['local_path'],'test_data.txt'))\n",
    "test_eq(test_config['local_path']+'/test_data.txt.2.5.0',\n",
    "        make_dataset_archive_folder(test_config['local_path'],'test_data.txt',['2.4.6'],'minor'))\n",
    "test_eq(test_config['local_path']+'/sub.0.0.1',\n",
    "        make_dataset_archive_folder(test_config['local_path'],'sub'))\n",
    "# TODO: check archive folder contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StorageClientABC(ABC):\n",
    "    \"\"\"Defines functionality common to all storage clients\"\"\"\n",
    "    \n",
    "    def __init__(self, config:dict):\n",
    "        \"Create a new storage client using the specified `config`\"\n",
    "        self.config=config\n",
    "        \n",
    "    def config_get(self,key,default=None,dtype=None):\n",
    "        \"Return a value via `self.config.get` optionally checking that the value is of `dtype`\"\n",
    "        result=self.config.get(key,default)\n",
    "        if dtype is not None and not isinstance(result,dtype): \n",
    "            raise ValueError(f'Config[{key}] should be a {dtype} but we found {result} which is a {type(result)}')\n",
    "        return result\n",
    "\n",
    "    def ls(self, what:str='storage_area',name_starts_with:str=None) -> List[str]:\n",
    "        \"Return a list containing the names of files in either `storage_area` or `local_path`\"\n",
    "        p=Path(self.config[what])\n",
    "        p.mkdir(parents=True,exist_ok=True)\n",
    "        len_p=len(str(p).replace('\\\\','/'))\n",
    "        result=[str(f).replace('\\\\','/')[len_p+1:] for f in p.rglob('*') if f.is_file()]\n",
    "        if name_starts_with is not None: \n",
    "            result=[r for r in result if r.startswith(name_starts_with)]\n",
    "        return sorted(result)\n",
    "        \n",
    "    @abstractmethod\n",
    "    def download(self, filename:str) -> Path: \n",
    "        \"Copy `filename` from `storage_area` to `local_path`\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def upload(self, filename:str, overwrite=False) -> Union[Path,str]: \n",
    "        \"Copy `filename` from `local_path` to `storage_area`\"\n",
    "        \n",
    "    def _sort_by_dataset_archive_version(self,version):\n",
    "        try: return tuple(parse_dataset_archive_version(version))\n",
    "        except: return (-1,-1,-1)\n",
    "        \n",
    "    def ls_versions(self, name:str, what:str='storage_area') -> Union[List[str],None]:\n",
    "        \"Return a list containing all versions of the specified archive `name`\"\n",
    "        files=[parse_dataset_archive_name(f) for f in self.ls(what)]\n",
    "        result=[f[1] for f in files if f is not None and f[0]==name]\n",
    "        if not result: return None\n",
    "        return sorted(result, key=self._sort_by_dataset_archive_version)\n",
    "        \n",
    "    def upload_dataset(self, name:str, version:str='patch') -> Union[Path,str]:\n",
    "        \"Create a new dataset archive and upload it to `storage_area`\"\n",
    "        archive_folder=make_dataset_archive_folder(\n",
    "                self.config['local_path'],name,self.ls_versions(name),version)\n",
    "        default_compression=zlib.Z_DEFAULT_COMPRESSION\n",
    "        compression=self.config_get('compression_level',default_compression,int)\n",
    "        try:\n",
    "            zlib.Z_DEFAULT_COMPRESSION=int(compression)\n",
    "            shutil.make_archive(archive_folder,'zip',archive_folder)\n",
    "        finally:\n",
    "            zlib.Z_DEFAULT_COMPRESSION=default_compression\n",
    "        return self.upload(f\"{archive_folder[len(self.config['local_path'])+1:]}.zip\")\n",
    "        \n",
    "    def download_dataset(self, name:str, version:str='latest', overwrite:bool=False) -> Path:\n",
    "        \"Download a dataset archive from `storage_area` and extract it to `local_path`\"\n",
    "        if version=='latest': \n",
    "            versions=self.ls_versions(name)\n",
    "            if versions is None:\n",
    "                raise ValueError('latest version requested but no versions exist in storage area')\n",
    "            version=versions[-1]\n",
    "        dst=Path(self.config['local_path'])/f'{name}.{version}'\n",
    "        if dst.exists():\n",
    "            if not overwrite: return dst\n",
    "            else: shutil.rmtree(dst)\n",
    "        archive=self.download(f'{name}.{version}.zip')\n",
    "        shutil.unpack_archive(str(archive),dst)\n",
    "        check_archive(dst)\n",
    "        return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"StorageClientABC.__init__\" class=\"doc_header\"><code>StorageClientABC.__init__</code><a href=\"__main__.py#L5\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>StorageClientABC.__init__</code>(**`config`**:`dict`)\n",
       "\n",
       "Create a new storage client using the specified `config`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(StorageClientABC.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"StorageClientABC.ls\" class=\"doc_header\"><code>StorageClientABC.ls</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>StorageClientABC.ls</code>(**`what`**:`str`=*`'storage_area'`*, **`name_starts_with`**:`str`=*`None`*)\n",
       "\n",
       "Return a list containing the names of files in either `storage_area` or `local_path`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(StorageClientABC.ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"StorageClientABC.ls_versions\" class=\"doc_header\"><code>StorageClientABC.ls_versions</code><a href=\"__main__.py#L38\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>StorageClientABC.ls_versions</code>(**`name`**:`str`, **`what`**:`str`=*`'storage_area'`*)\n",
       "\n",
       "Return a list containing all versions of the specified archive `name`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(StorageClientABC.ls_versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"StorageClientABC.upload_dataset\" class=\"doc_header\"><code>StorageClientABC.upload_dataset</code><a href=\"__main__.py#L45\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>StorageClientABC.upload_dataset</code>(**`name`**:`str`, **`version`**:`str`=*`'patch'`*)\n",
       "\n",
       "Create a new dataset archive and upload it to `storage_area`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(StorageClientABC.upload_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `name`\n",
    "    - file or folder name\n",
    "    - which must exist in \"local_path\"\n",
    "    - without \"local_path\" prefix\n",
    "    - e.g. if\n",
    "        - \"local_path\" is \"~/storage_tools/test/local_path\"\n",
    "        - and you want to upload \"~/storage_tools/test/local_path/test_data.txt\" as a dataset\n",
    "        - you would pass the name \"test_data.txt\"\n",
    "- `version`\n",
    "    - \"major\", \"minor\" or \"patch\" to automatically create a new version or\n",
    "    - version literal that matches `\\d+\\.\\d+\\.\\d+` (e.g. \"1.0.45\")\n",
    "\n",
    "<code>upload_dataset</code> will;\n",
    "- create a folder `[local_path]/[name].[version]`\n",
    "    - if this folder already exists, as error will be raised\n",
    "- copy the file or folder contents (and all sub-folders) to this folder\n",
    "- create a manifest in this folder\n",
    "- create a zip archive, called `[name].[version].zip`, of this folder\n",
    "    - Use the optional `compression_level` config setting to control the level of compression used by `zlib`\n",
    "    - If you are creating a dataset of files that are already compressed (e.g. images, models etc), setting `compression_level=0` can make this step run much faster\n",
    "- upload the zip archive to remote storage\n",
    "- return the location of the dataset in remote storage\n",
    "\n",
    "Why no `overwrite` option?\n",
    "- It is not expected that archives will need to be overwritten\n",
    "    - as we want to be able to re-run old experiments using the data as it was\n",
    "- bad archives could be deleted via storage API (e.g. `storage_client.client.delete_blob('test.0.0.1.zip')`) or via storage browsers\n",
    "    - we might want to add a soft delete, archive status etc to handle this kind of thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"StorageClientABC.download_dataset\" class=\"doc_header\"><code>StorageClientABC.download_dataset</code><a href=\"__main__.py#L58\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>StorageClientABC.download_dataset</code>(**`name`**:`str`, **`version`**:`str`=*`'latest'`*, **`overwrite`**:`bool`=*`False`*)\n",
       "\n",
       "Download a dataset archive from `storage_area` and extract it to `local_path`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(StorageClientABC.download_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `name`\n",
    "    - dataset name\n",
    "- `version`\n",
    "    - \"latest\" download the latest version of the dataset\n",
    "    - version literal that matches `\\d+\\.\\d+\\.\\d+` (e.g. \"1.0.45\")\n",
    "- `overwrite`\n",
    "    - If `False` and the dataset exists in \"local_path\", this is a no op\n",
    "    - If `True` and the dataset exists in \"local_path\", delete the dataset and re-download\n",
    "    \n",
    "<code>download_dataset</code> will;\n",
    "- download a zip archive, called `[name].[version].zip`, from remote storage\n",
    "- extract it to `[local_path]/[name].[version]`\n",
    "- check that all files listed in manifest.json have the correct secure hash\n",
    "- return the location of the dataset in local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LocalStorageClient(StorageClientABC):\n",
    "    \"\"\"Storage client that uses the local filesystem for both `storage_area` and `local_path`\"\"\"\n",
    "    \n",
    "    def _cp(self,from_key,to_key,filename,overwrite=False):\n",
    "        src=Path(self.config[from_key])/filename\n",
    "        dst=Path(self.config[to_key])/filename\n",
    "        if dst.exists() and not overwrite: \n",
    "            raise FileExistsError(f'{dst} exists and overwrite=False')\n",
    "        dst.parent.mkdir(parents=True,exist_ok=True)\n",
    "        shutil.copy(src,dst)\n",
    "        return dst\n",
    "        \n",
    "    def download(self,filename,overwrite=False):\n",
    "        try: self._cp('storage_area','local_path',filename,overwrite)\n",
    "        except FileExistsError: pass\n",
    "        return Path(self.config['local_path'])/filename\n",
    "        \n",
    "    def upload(self,filename,overwrite=False): \n",
    "        return self._cp('local_path','storage_area',filename,overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LocalStorageClient` will most often be used for local testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client=LocalStorageClient(test_config)\n",
    "assert storage_client.config['storage_client']=='storage_tools.core.LocalStorageClient'\n",
    "test_fail(lambda: storage_client.config['bad_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"StorageClientABC.config_get\" class=\"doc_header\"><code>StorageClientABC.config_get</code><a href=\"__main__.py#L9\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>StorageClientABC.config_get</code>(**`key`**, **`default`**=*`None`*, **`dtype`**=*`None`*)\n",
       "\n",
       "Return a value via `self.config.get` optionally checking that the value is of `dtype`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(StorageClientABC.config_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert storage_client.config_get('storage_client')=='storage_tools.core.LocalStorageClient'\n",
    "assert storage_client.config_get('bad_key')==None\n",
    "assert storage_client.config_get('storage_client',dtype=str)=='storage_tools.core.LocalStorageClient'\n",
    "test_fail(lambda: storage_client.config_get('storage_client',dtype=int))\n",
    "assert storage_client.config_get('bad_key','but its ok')=='but its ok'\n",
    "assert storage_client.config_get('storage_client','default is not used')=='storage_tools.core.LocalStorageClient'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AzureStorageClient(StorageClientABC):\n",
    "    \"\"\"Storage client that uses Azure for `storage_area` and the local filesystem `local_path`\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def client(self):\n",
    "        if not hasattr(self,'_client'):\n",
    "            service_client=azure.storage.blob.BlobServiceClient.from_connection_string(\n",
    "                self.config['conn_str'],self.config['credential'])\n",
    "            self._client=service_client.get_container_client(self.config['container'])\n",
    "        return self._client\n",
    "    \n",
    "    def ls(self,what='storage_area',name_starts_with=None):\n",
    "        if what=='local_path': return super().ls(what,name_starts_with)\n",
    "        result=[b.name for b in self.client.list_blobs(name_starts_with)]\n",
    "        return sorted(result)\n",
    "    \n",
    "    def download(self,filename,overwrite=False):\n",
    "        p=Path(self.config['local_path'])/filename\n",
    "        if p.exists() and not overwrite: return p\n",
    "        p.parent.mkdir(parents=True,exist_ok=True)\n",
    "        with open(p, 'wb') as f:\n",
    "            f.write(self.client.download_blob(filename).readall())\n",
    "        return p\n",
    "            \n",
    "    def upload(self,filename,overwrite=False): \n",
    "        p=Path(self.config['local_path'])/filename\n",
    "        try:\n",
    "            with open(p, 'rb') as f:\n",
    "                self.client.upload_blob(filename,f,overwrite=overwrite)\n",
    "            return f\"{self.config['storage_client']}:{self.config['container']}:{filename}\"\n",
    "        except azure.core.exceptions.ResourceExistsError as e:\n",
    "            raise FileExistsError(f'{e}\\noverwrite=False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AwsStorageClient(StorageClientABC):\n",
    "    \"\"\"Storage client that uses AWS for `storage_area` and the local filesystem `local_path`\"\"\"\n",
    "\n",
    "    @property\n",
    "    def client(self):\n",
    "        if not hasattr(self,'_client'):\n",
    "            self._client=boto3.client(service_name=self.config['service_name'],\n",
    "                                      aws_access_key_id=self.config['aws_access_key_id'],\n",
    "                                      aws_secret_access_key=self.config['aws_secret_access_key'])\n",
    "        return self._client\n",
    "    \n",
    "    def ls(self,what='storage_area',name_starts_with=None): \n",
    "        if what=='local_path': return super().ls(what,name_starts_with)\n",
    "        args=dict(Bucket=self.config['bucket'])\n",
    "        if name_starts_with is not None: args['Prefix']=name_starts_with\n",
    "        objects=self.client.list_objects_v2(**args)\n",
    "        if objects['KeyCount']==0: return []\n",
    "        result=[o['Key'] for o in objects['Contents'] if o['Size']>0]\n",
    "        return sorted(result)\n",
    "    \n",
    "    def download(self,filename,overwrite=False): \n",
    "        p=Path(self.config['local_path'])/filename\n",
    "        if p.exists() and not overwrite: return p\n",
    "        p.parent.mkdir(parents=True,exist_ok=True)\n",
    "        self.client.download_file(\n",
    "                Filename='/'.join([self.config['local_path'],filename]),\n",
    "                Bucket=self.config['bucket'],\n",
    "                Key=filename)\n",
    "        return p\n",
    "        \n",
    "    def upload(self,filename,overwrite=False): \n",
    "        result=f\"{self.config['storage_client']}:{self.config['bucket']}:{filename}\"\n",
    "        if overwrite==False and filename in [self.ls(name_starts_with=filename)]:\n",
    "            raise FileExistsError(f'{result} exists and overwrite=False')\n",
    "        self.client.upload_file(\n",
    "                Filename='/'.join([self.config['local_path'],filename]),\n",
    "                Bucket=self.config['bucket'],\n",
    "                Key=filename)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would use the same property keys in settings.ini as the boto3 API but `ConfigParser` converts keys to lower case by default.\n",
    "\n",
    "So if boto3 has a parameter called `Bucket`, we need `bucket=a-bucket-name` settings.ini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "If we pass `None` to `Prefix` when listing objects (e.g. `list_objects_v2(Prefix=None)`) AWS raises an error.\n",
    "This is why we have to create and unpack the `args` dictionary when we call `list_objects_v2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def new_storage_client(storage_name:str,config_name:str='secrets/settings.ini'):\n",
    "    \"Returns a storage client based on the configured `storage_client`\"\n",
    "    try:\n",
    "        config=read_config(storage_name,config_name=config_name)\n",
    "        storage_client=config['storage_client']\n",
    "        module=importlib.import_module(storage_client[:storage_client.rindex('.')])\n",
    "        return getattr(module,storage_client[storage_client.rindex('.')+1:])(config)\n",
    "    except Exception as ex:\n",
    "        message=f'Failed to create storage client. storage_name={storage_name}, config_name={config_name}'\n",
    "        raise ValueError(message) from ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function reads the `storage_name` section of `config_name`. \n",
    "\n",
    "A key config setting is `storage_client` which should;\n",
    "- specify the storage client implementation to use (fully qualified with package name)\n",
    "- refer to a module that can be imported\n",
    "- refer to a class that is a `StorageClientABC` (that has not changed the signature of `__init__`)\n",
    "\n",
    "This makes it possible to use a storage client implementation that is not defined in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client=new_storage_client('local_test','test/settings.ini')\n",
    "assert storage_client.config['storage_client']=='storage_tools.core.LocalStorageClient'\n",
    "test_fail(lambda: new_storage_client('gcp_dummy','test/settings.ini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in [test_config['local_path'],test_config['storage_area']]: _rmtree(p)\n",
    "\n",
    "storage_client=LocalStorageClient(test_config)\n",
    "assert isinstance(storage_client,LocalStorageClient)\n",
    "assert storage_client.config['storage_client']=='storage_tools.core.LocalStorageClient'\n",
    "test_eq([],storage_client.ls())\n",
    "test_eq([],storage_client.ls('local_path'))\n",
    "    \n",
    "test_files=_make_local_test_data()\n",
    "test_eq([],storage_client.ls())\n",
    "test_eq(test_files,storage_client.ls('local_path'))\n",
    "test_eq(['a/b/test_data.2.0.0.txt'],storage_client.ls('local_path','a/b'))\n",
    "test_eq([],storage_client.ls('local_path','does/not/exist'))\n",
    "        \n",
    "for f in test_files: storage_client.upload(f)\n",
    "test_eq(test_files,storage_client.ls())\n",
    "test_eq(['a/b/test_data.2.0.0.txt'],storage_client.ls(name_starts_with='a/b'))\n",
    "test_eq([],storage_client.ls(name_starts_with='does_no_exist'))\n",
    "test_eq(test_files,storage_client.ls('local_path'))\n",
    "_rmtree(test_config['local_path'])\n",
    "test_eq([],storage_client.ls('local_path'))\n",
    "\n",
    "for f in test_files: storage_client.download(f)\n",
    "test_eq(test_files,storage_client.ls('local_path'))\n",
    "test_eq('a little bit of data 4',open(test_config['local_path']+'/test_data.txt').read())\n",
    "\n",
    "with open(test_config['local_path']+'/test_data.txt', 'w') as _file: _file.write('upd')\n",
    "test_eq('upd',open(test_config['local_path']+'/test_data.txt').read())\n",
    "storage_client.download('test_data.txt')\n",
    "test_eq('upd',open(test_config['local_path']+'/test_data.txt').read())\n",
    "storage_client.download('test_data.txt',True)\n",
    "test_eq('a little bit of data 4',open(test_config['local_path']+'/test_data.txt').read())\n",
    "\n",
    "test_fail(lambda: storage_client.upload('test_data.txt'))\n",
    "storage_client.upload('test_data.txt',True)\n",
    "\n",
    "test_eq(None,storage_client.ls_versions('this/does/not/exitst'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in [test_config['local_path'],test_config['storage_area']]: _rmtree(p)\n",
    "    \n",
    "storage_client=LocalStorageClient(test_config)\n",
    "test_files=_make_local_test_data()\n",
    "\n",
    "def _t(expected,upload_name,version='patch'):\n",
    "    test_eq(Path(test_config['storage_area'])/expected,storage_client.upload_dataset(upload_name,version))\n",
    "_t('test_data.txt.0.0.1.zip','test_data.txt')\n",
    "_t('sub.0.0.1.zip','sub')\n",
    "_t('sub/test_data.0.0.2.txt.0.0.1.zip','sub/test_data.0.0.2.txt')\n",
    "# switch off compression when creating zip archives\n",
    "test_config['compression_level']=0\n",
    "_t('a.3.0.0.zip','a','3.0.0')\n",
    "_t('a.3.0.1.zip','a')\n",
    "# TODO: check zip contents\n",
    "_rmtree(test_config['local_path']+'/a.3.0.0')\n",
    "_rmtree(test_config['local_path']+'/a.3.0.1')\n",
    "test_eq(Path(test_config['local_path'])/'a.3.0.1',storage_client.download_dataset('a'))\n",
    "test_eq(Path(test_config['local_path'])/'a.3.0.0',storage_client.download_dataset('a','3.0.0'))\n",
    "test_eq(Path(test_config['local_path'])/'a.3.0.0',storage_client.download_dataset('a','3.0.0'))\n",
    "test_eq(Path(test_config['local_path'])/'a.3.0.0',storage_client.download_dataset('a','3.0.0',True))\n",
    "# check manifest was created and downloaded as part of the dataset\n",
    "assert Path(test_config['local_path']+'/a.3.0.0/manifest.json').is_file()\n",
    "check_archive(test_config['local_path']+'/a.3.0.0')\n",
    "# if we change the secure hash for any of the files in the dataset, check_archive should fail\n",
    "manifest=json.load(open(test_config['local_path']+'/a.3.0.0/manifest.json'))\n",
    "manifest['files'][0]['sha256']='thisllnevermatch'\n",
    "json.dump(manifest,open(test_config['local_path']+'/a.3.0.0/manifest.json','w'))\n",
    "test_fail(lambda: check_archive(test_config['local_path']+'/a.3.0.0'))\n",
    "# only files listed in the manifest are checked\n",
    "manifest['files']=[]\n",
    "json.dump(manifest,open(test_config['local_path']+'/a.3.0.0/manifest.json','w'))\n",
    "check_archive(test_config['local_path']+'/a.3.0.0')\n",
    "# check_archive will fail if it can't find a manifest\n",
    "Path(test_config['local_path']+'/a.3.0.0/manifest.json').unlink()\n",
    "test_fail(lambda: check_archive(test_config['local_path']+'/a.3.0.0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client=new_storage_client('azure_dummy','test/settings.ini')\n",
    "test_eq(storage_client.__class__.__name__,'AzureStorageClient')\n",
    "storage_client=new_storage_client('aws_dummy','test/settings.ini')\n",
    "test_eq(storage_client.__class__.__name__,'AwsStorageClient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean-up test data\n",
    "for p in [test_config['local_path'],test_config['storage_area']]: _rmtree(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
